# Portfolio 

This is the progress of my work and projects I have completed as of now. I do hope to add more projects in my professional portfolio.
------------------------------------------------------------------

### European Championship 2016 - SQL Database

I was told that I needed to learn SQL. Hence, I did a short course on SQL on UDACITY. 

I then used the Euro 2016 database to practice my SQL querying and joins.

[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://github.com/vinothnk/European-Football-Database)


------------------------------------------------------------------

### Data Science Modelling

In my final module in Data Science, we learnt about ***K-Means clustering, PCA and ML Train-Test Algorithms***. 

For our project, we were advised to apply our knowledge of what we learnt throughout our time in SUTD onto a dataset.

For purposes of achieving a better understanding, we were advised to use previous datasets. We continued with the IBM HR data.

[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://vinothnk.github.io/Data-Science-Modelling/)


Below is the pipeline we followed for our capstone project for the ***ModularMaster Certificate in Data Science*** course.
<center><img src="https://www.aismartz.com/blog/wp-content/uploads/2019/11/Electronic-Design-Automation-data-science-model.jpg"/></center>

------------------------------------------------------------------

### Data Validation and Statistical Analysis

Statistics are integral for a data analyst/scientist as explained by my lecturer. He mentioned that one may kick off a DS project with a judgemental approach to deciding which approach or technique will suit the project.

The use of statistics is an alternative to make DS a more systematic yet organised approach in terms of being critical and getting objective, data-driven scientific enquiries.

With stats, the data could be validated and required analysis can be conducted.

My group selected the IBM HR data as our dataset.

[![View on Kaggle](https://img.shields.io/badge/Kaggle-View%20on%20Kaggle-orange)](https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset)

------------------------------------------------------------------

***Parametric Test between Monthly Income and Gender***

We were allowed to use any dataset for this project. We chose Human Resources as one of my group mate was from a HR department and he would have more insight as to how we can use HR data.

We decided to go with Montly Income and Gender as our variables for analysis if there was a disparity between Monthly Income and your gender.

[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://vinothnk.github.io/Data-Validation-Statistical-Analysis---Parametric/)

------------------------------------------------------------------

***Non-Parametric Test between WorkLifeBalance and MonthlyIncome***

Given that everyone craves and wanted work life balance, we focused on WorkLifeBalance and MonthlyIncome as our variables for the Non-Parametric tests.

The results should let us know if there is any work life balance based on your monthly income.

[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://vinothnk.github.io/Data-Validation-Statistical-Analysis-Non-Parametric/)

---

### Data Wrangling with Programming

For my next module, we learnt to use Python to perform the following actions :
- data discovery
- data structuring
- data cleaning
- data validation
- data enrichment
- data aggregation

Our group project has 2 parts. 

The first was to clean a dataset which was structured. Meaning that data was either in table format or from a csv/excel file format
which would make it easier to clean.

The second is to clean UNSTRUCTURED data. Examples of such data are Text files, Email, social media, media, mobile data, etc.


***E-Commerce dataset - Structured***

We found the dataset on Kaggle. It was Pakistan's largest e-commerce data. We used this dataset as 1 of the requirement for our project was that
dataset consisted of 100k to 10million rows minimum. This dataset had about 500k rows which fulfilled our project requirements.

[![View on Kaggle](https://img.shields.io/badge/Kaggle-View%20on%20Kaggle-orange)](https://www.kaggle.com/datasets/zusmani/pakistans-largest-ecommerce-dataset)
[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://vinothnk.github.io/Data-Wrangling-with-Programming/)

<center><img src="https://user-images.githubusercontent.com/108440564/179391657-b548ee79-3085-46de-ba72-f0b149871e2a.png"/></center>



***Enron Emails - Un-Structured***

For our un-structured dataset, we decided to go with emails. The reason why we decided to go with emails was because we wanted to perform sentiment analysis using the 
***NLTK*** package. And we had the requirement of minimum 1k to 100k rows to fulfill. 

After doing our research, we found the Enron emails dataset with around 500k emails. That should be sufficient to clean, structure and to analyse the data for sentiment analysis.

[![View on Kaggle](https://img.shields.io/badge/Kaggle-View%20on%20Kaggle-orange)](https://www.kaggle.com/datasets/wcukierski/enron-email-dataset)
[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://vinothnk.github.io/Data-Wrangling-with-Programming-2/)
<center><img src="https://storage.googleapis.com/kaggle-datasets-images/55/110/b720c0cc3f965a27a2b2bfae3c815f1f/dataset-card.png"/></center>

------------------------------------------------------------------

### Data Visualisation

In my class for Data Visualisation, we were taught to use visualisation tools like Tableau and PowerBI.

For my group project, we decide to do a visualisation on the effects of Covid throughout the world.

[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://github.com/vinothnk/Data-Storytelling-with-Visualisation) 
[![View on Tableau](https://img.shields.io/badge/Tableau-View%20on%20Tableau%20Public-brightgreen)](https://public.tableau.com/views/CovidDataVisualisation_16579515462230/Storyline?:language=en-GB&publish=yes&:display_count=n&:origin=viz_share_link)


<center><img src="https://assets.entrepreneur.com/content/3x2/2000/20190909181947-sale-21635-primary-image-wide.jpeg"/></center>

------------------------------------------------------------------

### Foundation of Data Science

As this is the introduction to Data Science, I learnt the following steps in the data cycle and how to perform data analysis using Excel.

- Step 1: Business Question
- Step 2: Data Acquisition
- Step 3: Data Preparation
- Step 4: Data Modelling
- Step 5: Data Visualisation
- Step 6: Business Decision

We will be covering Steps 1 till Step 4 in this module.

[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://github.com/vinothnk/Foundation-of-Data-Science)

<center><img src="https://www.analytixlabs.co.in/blog/wp-content/uploads/2021/03/blogs-banner-7c-01-scaled-1024x724.jpg"/></center>

------------------------------------------------------------------

## Python

### Udemy - 100 Days of Coding by Angela Yu

An ongoing process in my journey to learning Python. Even though I had learnt python during my course in SUTD, working full time whilst studying part time did not allow me to fully concentrate on the lessons, with family commitments on the side as well as Covid. Hence, I decided to restart learning python to ensure I understand the basic principles of python. 

Below is the link of the repository of my 100 days of coding progress.

[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://github.com/vinothnk/100-Days-of-Coding)

<center><img src="images/100daysofpython.jpg"/></center>
